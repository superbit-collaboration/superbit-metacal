import logging
import os
import sys
import yaml
import re
from pathlib import Path
from astropy.table import Table
from numpy.random import SeedSequence, default_rng
import time
import numpy as np
import subprocess
from astropy.io import fits
import astropy.wcs as wcs

import pdb

class ForkedPdb(pdb.Pdb):
    """A Pdb subclass that may be used
    from a forked multiprocessing child
    https://stackoverflow.com/questions/4716533/how-to-attach-debugger-to-a-python-subproccess
    """
    def interaction(self, *args, **kwargs):
        _stdin = sys.stdin
        try:
            sys.stdin = open('/dev/stdin')
            pdb.Pdb.interaction(self, *args, **kwargs)
        finally:
            sys.stdin = _stdin

        return

class LogPrint(object):

    def __init__(self, log, vb):
        '''
        Requires a logging obj and verbosity level
        '''

        # Must be either a Logger object or None
        if log is not None:
            if not isinstance(log, logging.Logger):
                raise TypeError('log must be either a Logger ' +\
                                'instance or None!')

        self.log = log
        self.vb = vb

        return

    def __call__(self, msg=None):
        '''
        treat it like print()
        e.g. lprint = LogPrint(...); lprint('message')
        '''

        if msg is None:
            msg = ''

        if self.log is not None:
            self.log.info(msg)
        if self.vb is True:
            print(msg)

        return

    def debug(self, msg):
        '''
        don't print for a debug
        '''
        self.log.debug(msg)

        return

    def warning(self, msg):
        self.log.warning(msg)
        if self.vb is True:
            print(msg)

        return

class Logger(object):

    def __init__(self, logfile, logdir=None):
        if logdir is None:
            logdir = './'

        self.logfile = os.path.join(logdir, logfile)

        # only works for newer versions of python
        # log = logging.basicConfig(filename=logfile, level=logging.DEBUG)

        # instead:
        log = logging.getLogger()
        log.setLevel(logging.INFO)
        # log.setLevel(logging.ERROR)
        handler = logging.FileHandler(self.logfile, 'w', 'utf-8')
        handler.setFormatter(logging.Formatter('%(name)s %(message)s'))
        log.addHandler(handler)

        self.log = log

        return

    # other useful things?
    # ...

def setup_logger(logfile, logdir=None):
    '''
    Utility function if you just want the log and not the Logger object
    '''

    if logdir is not None:
        if not os.path.exists(logdir):
            os.makedirs(logdir)

    logger = Logger(logfile, logdir=logdir)

    return logger.log

def read_yaml(yaml_file):
    '''
    current package has a problem reading scientific notation as
    floats; see
    https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number
    '''

    loader = yaml.SafeLoader
    loader.add_implicit_resolver(
        u'tag:yaml.org,2002:float',
        re.compile(u'''^(?:
        [-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+]?[0-9]+)?
        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)
        |\\.[0-9_]+(?:[eE][-+][0-9]+)?
        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*
        |[-+]?\\.(?:inf|Inf|INF)
        |\\.(?:nan|NaN|NAN))$''', re.X),
        list(u'-+0123456789.'))

    with open(yaml_file, 'r') as stream:
        # return yaml.safe_load(stream) # see above issue
        return yaml.load(stream, Loader=loader)

def write_yaml(yaml_dict, yaml_outfile):
    '''
    yaml_dict: dict
        The dictionary to save to a yaml config file
    yaml_outfile: str
        The path of the output config file
    '''

    with open(yaml_outfile, 'w') as yaml_file:
        yaml.dump(yaml_dict, yaml_file, default_flow_style=False)

    return

def generate_seeds(Nseeds, master_seed=None, seed_bounds=(0, 2**32-1),
                   return_master=False):
    '''
    generate a set of safe, independent seeds given a master seed

    Nseeds: int
        The number of desired independent seeds
    master_seed: int
        A seed that initializes the SeedSequence, if desired
    seed_bounds: tuple of ints
        The min & max values for the seeds to be sampled from
    return_master: bool
        Return the master seed as well (in case it wasn't passed!)
    '''

    if (not isinstance(Nseeds, int)) or (Nseeds < 1):
        raise ValueError('Nseeds must be a positive int!')

    for b in seed_bounds:
        if not isinstance(b, int):
            raise TypeError('seed_bounds must be a tuple of ints!')
        if seed_bounds[0] < 0:
            raise ValueError('seed_bounds must be positive!')
        if seed_bounds[1] < seed_bounds[0]:
            raise ValueError('seed_bounds values must be monotonic!')

    if master_seed is None:
        # local time in microseconds
        master_seed = int(time.time()*1e6)

    ss = SeedSequence(master_seed)
    child_seeds = ss.spawn(Nseeds)
    streams = [default_rng(s) for s in child_seeds]

    seeds = []
    for k in range(Nseeds):
        val = int(streams[k].integers(seed_bounds[0], seed_bounds[1]))
        seeds.append(val)

    if return_master is True:
        return seeds, master_seed
    else:
        return seeds

def check_req_params(config, params, defaults):
    '''
    Ensure that certain required parameters have their values set to
    something either than the default after a configuration file is read.
    This is needed to allow certain params to be set either on the command
    line or config file.

    config: An object that (potentially) has the param values stored as
    attributes
    params: List of required parameter names
    defaults: List of default values of associated params
    '''

    for param, default in zip(params, defaults):
        # Should at least be set by command line arg defaults, but double check:
        if (not hasattr(config, param)) or (getattr(config, param) == default):
            e_msg = f'Must set {param} either on command line or in passed config!'
            raise Exception(e_msg)

    return

def check_req_fields(config, req, name=None):
    for field in req:
        if not field in config:
            raise ValueError(f'{name}config must have field {field}')

    return

def parse_config(config, req, opt, name=None, allow_unregistered=False):
    '''
    config: dict
        A configuration dictionary
    req: list, dict
        A list of required field names
    opt: dict
        A dictionary of optional field names, with
        optional values assigned
    name: str
        Name of config type, for extra print info
    allow_unregistered: bool
        Set to allow fields not registered as a req or optional field
    '''

    if (config is not None) and (not isinstance(config, dict)):
        raise TypeError('config must be a dict!')
    if (req is not None) and (not isinstance(req, list)):
        raise TypeError('req must be a list or dict!')
    if (opt is not None) and (not isinstance(opt, dict)):
        raise TypeError('opt must be a dict!')

    if name is None:
        name = ''
    else:
        name = name + ' '

    if req is None:
        req = []
    if opt is None:
        opt = {}

    # ensure all req fields are present
    check_req_fields(config, req, name=name)

    # now check for fields not in either
    if allow_unregistered is False:
        for field in config:
            if (not field in req) and (not field in opt):
                raise ValueError(f'{field} not a valid field for {name}config!')

    # set defaults for any optional field not present in config
    for field, value in opt.items():
        if field not in config:
            config[field] = value

    return config

def check_type(name, arg, allowed_types):
    '''
    Check if the passed arg is of the allowed types

    name: str
        Name of the arg
    arg: any
        The arg in question
    allowed_types: type; tuple of types
        The required type or tuple of allowed types
    '''

    if isinstance(allowed_types, tuple):
        err_msg = f'{name} must be one of {allowed_types}!'
    else:
        err_msg = f'{name} must be a {allowed_types}!'

    if not isinstance(arg, allowed_types):
        raise TypeError(err_msg)

    return

def sigma2fwhm(sigma):
    c = np.sqrt(8.*np.log(2))
    return c * sigma

def fwhm2sigma(fwhm):
    c = np.sqrt(8.*np.log(2))
    return fwhm / c

def decode(msg):
    if isinstance(msg, str):
        return msg
    elif isinstance(msg, bytes):
        return msg.decode('utf-8')
    elif msg is None:
        return ''
    else:
        print(f'Warning: message={msg} is not a string or bytes')
        return msg

def run_command(cmd, logprint=None, silent=False):

    if logprint is None:
        # Just remap to print then
        logprint = print

    args = [cmd.split()]
    kwargs = {
        'stdout':subprocess.PIPE,
        'stderr':subprocess.STDOUT,
        'bufsize':1
        }

    with subprocess.Popen(*args, **kwargs) as process:
        try:
            # for line in iter(process.stdout.readline, b''):
            for line in iter(process.stdout.readline, b''):
                if silent is False:
                    logprint(decode(line).replace('\n', ''))

            stdout, stderr = process.communicate()

        except:
            if silent is False:
                logprint('')
                logprint('.....................ERROR....................')
                logprint('')

                logprint('\n'+decode(stderr))

            rc = process.poll()
            raise subprocess.CalledProcessError(
                rc,
                process.args,
                output=stdout,
                stderr=stderr
                )

        rc = process.poll()


        if rc:
            stdout, stderr = process.communicate()
            if silent is False:
                logprint('\n'+decode(stderr))
            raise subprocess.CalledProcessError(
                rc,
                process.args,
                output=stdout,
                stderr=stderr
                )

    return rc

def ngmix_dict2table(d):
    '''
    convert the result of a ngmix fit to an astropy table
    '''

    # Annoying, but have to do this to make Table from scalars
    for key, val in d.items():
        d[key] = np.array([val])

    return Table(data=d)

def setup_batches(nobjs, ncores):
    '''
    Create list of batch indices for each core
    '''

    if ncores >= 1:
        batch_len = [nobjs//ncores]*(ncores-1)
    else:
        raise ValueError('ncores must be >= 1')

    s = int(np.sum(batch_len))
    batch_len.append(nobjs-s)

    batch_indices = []

    start = 0
    for i in range(ncores):
        batch_indices.append(range(start, start + batch_len[i]))
        start += batch_len[i]

    return batch_indices

def get_pixel_scale(image_filename):
    '''
    use astropy.wcs to obtain the pixel scale (a/k/a plate scale)
    for the input image. Returns pixel scale in arcsec/pixels.

    image_filename: str
        FITS image for which pixel scale is desired

    Return:
    pix_scale: float
        Image pixel scale in arcsec/pixel
    '''

    # Get coadd image header
    hdr = fits.getheader(image_filename)

    # Instantiate astropy.wcs.WCS header
    w = wcs.WCS(hdr)

    # Obtain pixel scale in degrees/pix & convert to arcsec/pix
    cd1_1 = wcs.utils.proj_plane_pixel_scales(w)[0]
    pix_scale = cd1_1 * 3600

    return pix_scale

def rm_tree(path):
    '''
    Recursive directory removal using pathlib

    path: str, pathlib.Path
        The directory path to recursively delete
    '''

    path = Path(path)
    for child in path.glob('*'):
        if child.is_file():
            child.unlink()
        else:
            rm_tree(child)

    path.rmdir()

    return

def make_dir(d):
    '''
    Makes dir if it does not already exist

    d: str, pathlib.Path
    '''

    if isinstance(d, Path):
        d = str(d.resolve())

    if not os.path.exists(d):
        os.makedirs(d)

    return

def get_base_dir():
    '''
    base dir is parent repo dir
    '''
    module_dir = get_module_dir()
    return os.path.dirname(module_dir)

def get_module_dir():
    return os.path.dirname(__file__)

def get_test_dir():
    base_dir = get_base_dir()
    return os.path.join(base_dir, 'tests')

BASE_DIR = get_base_dir()
MODULE_DIR = get_module_dir()
TEST_DIR = get_test_dir()
